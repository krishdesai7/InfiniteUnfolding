{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d29cdea1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "195863ba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', size=10)\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import wasserstein_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46398d77",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "#load and normalize the data\n",
    "data = np.load('rawdata.npz')\n",
    "substructure_variables = ['w', 'q', 'm', 'r', 'tau1s', 'tau2s']\n",
    "data_streams = ['_true', '_true_alt', '_reco', '_reco_alt']\n",
    "n_variables = len(substructure_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d7b5cba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "normalize = True\n",
    "    \n",
    "for var_name in data.files:\n",
    "    globals()[var_name] = data[var_name]\n",
    "    \n",
    "if normalize:\n",
    "    for var_name in substructure_variables:\n",
    "        mu = np.mean(globals()[var_name+data_streams[0]])\n",
    "        sig = np.std(globals()[var_name + data_streams[0]])\n",
    "        for stream in data_streams:\n",
    "            globals()[var_name+stream] = (globals()[var_name+stream] - mu)/sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "633852cd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "N = len(m_true)\n",
    "\n",
    "xvals_truth = np.array([np.concatenate([globals()[f\"{var_name}_true_alt\"], globals()[f\"{var_name}_true\"]]) for var_name in substructure_variables]).T\n",
    "xvals_reco = np.array([np.concatenate([globals()[f\"{var_name}_reco_alt\"], globals()[f\"{var_name}_reco\"]]) for var_name in substructure_variables]).T\n",
    "                    \n",
    "yvals = np.concatenate([np.zeros(N, dtype=np.float32),np.ones(N, dtype=np.float32)])\n",
    "\n",
    "X_train_truth, X_test_truth, X_train_reco, X_test_reco, Y_train, Y_test = train_test_split(\n",
    "    xvals_truth, xvals_reco, yvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a050bea1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.get_custom_objects().clear()\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\", name=\"weighted_wgan_critic_loss\")\n",
    "def weighted_wgan_critic_loss(target, output, weights):\n",
    "    target = tf.convert_to_tensor(target, dtype=tf.float32)\n",
    "    output = tf.convert_to_tensor(output, dtype=tf.float32)\n",
    "    weights = tf.convert_to_tensor(weights, dtype=tf.float32)\n",
    "    \n",
    "    # Separate weights for real and fake samples\n",
    "    weights_real = weights * target\n",
    "    weights_fake = weights * (1 - target)\n",
    "    \n",
    "    # Calculate the weighted scores for real and fake samples\n",
    "    weighted_scores_real = output * weights_real\n",
    "    weighted_scores_fake = output * weights_fake\n",
    "    \n",
    "    # Calculate the sum of weights for normalization\n",
    "    sum_weights_real = tf.reduce_sum(weights_real) + tf.keras.backend.epsilon()\n",
    "    sum_weights_fake = tf.reduce_sum(weights_fake) + tf.keras.backend.epsilon()\n",
    "    \n",
    "    # Calculate the weighted average scores\n",
    "    weighted_average_real = tf.reduce_sum(weighted_scores_real) / sum_weights_real\n",
    "    weighted_average_fake = tf.reduce_sum(weighted_scores_fake) / sum_weights_fake\n",
    "    \n",
    "    # WGAN critic loss is the difference between weighted averages\n",
    "    wgan_critic_loss = weighted_average_fake - weighted_average_real\n",
    "    \n",
    "    return wgan_critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "450314f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "gen_model_width = 150\n",
    "gen_model_depth = 8\n",
    "disc_model_width = 75\n",
    "disc_model_depth = 5\n",
    "dropout_rate = 0.2\n",
    "kernel_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.2)\n",
    "\n",
    "\n",
    "# Generator model\n",
    "def build_generator(input_shape):\n",
    "    gen_input = tf.keras.layers.Input(shape=(input_shape,))\n",
    "    x = gen_input\n",
    "        \n",
    "    for _ in range(gen_model_depth):\n",
    "        x = tf.keras.layers.Dense(gen_model_width, use_bias=False, kernel_initializer=kernel_init if _ == 0 else None)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(negative_slope=0.2)(x)\n",
    "        x = tf.keras.layers.LayerNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(1, use_bias=False, activation='relu')(x)\n",
    "\n",
    "    return tf.keras.models.Model(inputs=gen_input, outputs=outputs)\n",
    "\n",
    "def build_critic(input_shape, disc_model_width=75, disc_model_depth=5, dropout_rate=0.2):\n",
    "    critic_inputs = tf.keras.layers.Input(shape=(input_shape,))\n",
    "    x = critic_inputs\n",
    "    \n",
    "    for _ in range(disc_model_depth):\n",
    "        x = tf.keras.layers.Dense(disc_model_width, use_bias=False, kernel_initializer=kernel_init if _ == 0 else None)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(negative_slope=0.2)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    outputs = tf.keras.layers.Dense(1, activation=None)(x)  # Linear activation for the critic output\n",
    "    \n",
    "    return tf.keras.models.Model(inputs=critic_inputs, outputs=outputs)\n",
    "\n",
    "# Create the models\n",
    "model_generator = build_generator(xvals_truth.shape[1])\n",
    "model_critic = build_critic(xvals_reco.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ec27a5e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(critic, real_samples, fake_samples, penalty_lambda=5.0):\n",
    "    batch_size = tf.shape(real_samples)[0]\n",
    "    alpha = tf.random.uniform([batch_size, 1], 0.0, 1.0)\n",
    "    \n",
    "    interpolated = real_samples + (alpha * (fake_samples - real_samples))\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated)\n",
    "        predictions = critic(interpolated, training=True)\n",
    "    \n",
    "    gradients = tape.gradient(predictions, [interpolated])[0]\n",
    "    gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gradients), axis=[1]) + 1e-12)\n",
    "    gradient_penalty = penalty_lambda * tf.reduce_mean(tf.square(gradients_norm - 1.0))\n",
    "    \n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80b0c66c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def sample_real_fake():\n",
    "    global X_test_detector, Y_test, batch_size\n",
    "    \n",
    "    real_indices = np.where(Y_test == 1)[0]\n",
    "    fake_indices = np.where(Y_test == 0)[0]\n",
    "    \n",
    "    sampled_real_indices = np.random.choice(real_indices, size=batch_size // 2, replace=False)\n",
    "    sampled_fake_indices = np.random.choice(fake_indices, size=batch_size // 2, replace=False)\n",
    "    \n",
    "    real_samples = X_test_reco[sampled_real_indices]\n",
    "    fake_samples = X_test_reco[sampled_fake_indices]\n",
    "    \n",
    "    return real_samples, fake_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e4f63fa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "initial_learning_rate = 1e-5\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1e5,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer_gen = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
    "optimizer_critic = tf.keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
    "\n",
    "@tf.function\n",
    "def train_step_critic(X_detector_batch, Y_batch, W_batch):\n",
    "    model_critic.trainable = True\n",
    "    model_generator.trainable = False\n",
    "    real_samples, fake_samples = sample_real_fake()\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model_critic(X_detector_batch, training=True)\n",
    "        wgan_loss = weighted_wgan_critic_loss(Y_batch, predictions, W_batch)\n",
    "        gradient_penalty = compute_gradient_penalty(model_critic, real_samples, fake_samples)\n",
    "        total_loss = wgan_loss + gradient_penalty\n",
    "\n",
    "    grads = tape.gradient(total_loss, model_critic.trainable_variables)\n",
    "    optimizer_critic.apply_gradients(zip(grads, model_critic.trainable_variables))\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "@tf.function(reduce_retracing=True)\n",
    "def train_step_gan(X_particle_batch, X_detector_batch, Y_batch):\n",
    "    model_critic.trainable = False\n",
    "    model_generator.trainable = True\n",
    "    with tf.GradientTape() as tape:\n",
    "        W_batch = model_generator(X_particle_batch, training=True) \n",
    "        W_batch = tf.where(Y_batch == 1, 1.0, tf.squeeze(W_batch))\n",
    "        critic_output = model_critic(X_detector_batch, training=False)  \n",
    "        loss = -1*weighted_wgan_critic_loss(Y_batch, critic_output, W_batch)\n",
    "    grads = tape.gradient(loss, model_generator.trainable_variables)\n",
    "    optimizer_gen.apply_gradients(zip(grads, model_generator.trainable_variables))\n",
    "    return loss\n",
    "crit_loss_avg = []\n",
    "gen_loss_avg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "823714b5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1723822425.573103  691339 service.cc:145] XLA service 0x7f3fec008890 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1723822425.573141  691339 service.cc:153]   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "I0000 00:00:1723822425.573144  691339 service.cc:153]   StreamExecutor device (1): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "I0000 00:00:1723822425.573147  691339 service.cc:153]   StreamExecutor device (2): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "I0000 00:00:1723822425.573149  691339 service.cc:153]   StreamExecutor device (3): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "I0000 00:00:1723822425.573151  691339 service.cc:153]   StreamExecutor device (4): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "I0000 00:00:1723822425.573153  691339 service.cc:153]   StreamExecutor device (5): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "I0000 00:00:1723822425.573155  691339 service.cc:153]   StreamExecutor device (6): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "I0000 00:00:1723822425.573157  691339 service.cc:153]   StreamExecutor device (7): Tesla V100-SXM2-16GB, Compute Capability 7.0\n",
      "I0000 00:00:1723822431.632297  691339 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Critic Loss: 4.99998664855957, Generator Loss: -2510.9560546875\n",
      "Epoch 2/3\n",
      "Epoch 2 completed. Critic Loss: 4.999986171722412, Generator Loss: -2587.50146484375\n",
      "Epoch 3/3\n",
      "Epoch 3 completed. Critic Loss: 4.9999871253967285, Generator Loss: -2533.7236328125\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = './InfiniteUnfolding/model_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Paths for the checkpoint files\n",
    "checkpoint_path_gen = os.path.join(checkpoint_dir, 'generator_epoch-{epoch:04d}.weights.h5')\n",
    "\n",
    "n_epochs = 3\n",
    "batch_size = X_train_reco.shape[0]//100\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_reco, X_train_truth, Y_train)).batch(batch_size)\n",
    "\n",
    "m = 1  # Number of generator updates\n",
    "n = 3 # Number of critic updates\n",
    "\n",
    "performance_metric = np.empty((n_epochs, n_variables))\n",
    "data_set_1 = X_test_truth[Y_test == 1]\n",
    "data_set_2 = X_test_truth[Y_test == 0]\n",
    "baseline = np.array([wasserstein_distance(data_set_1[:, i], data_set_2[:, i]) for i in range(n_variables)])\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "    critic_losses = []\n",
    "    generator_losses = []\n",
    "\n",
    "    for X_detector_batch, X_particle_batch, Y_batch in train_dataset:\n",
    "        # Critic update loop\n",
    "        for _ in range(n):\n",
    "            model_generator.trainable = False\n",
    "            model_critic.trainable = True\n",
    "            W_batch = model_generator.predict(X_particle_batch, verbose=0)\n",
    "            W_batch = tf.where(Y_batch == 1, 1.0, tf.squeeze(W_batch))\n",
    "            d_loss = train_step_critic(X_detector_batch, Y_batch, W_batch)\n",
    "            critic_losses.append(d_loss.numpy())\n",
    "\n",
    "        # Generator update loop\n",
    "        for _ in range(m):\n",
    "            alt_indices = tf.where(Y_batch == 0)\n",
    "            X_particle_alt = tf.gather_nd(X_particle_batch, alt_indices)\n",
    "            X_detector_alt = tf.gather_nd(X_detector_batch, alt_indices)\n",
    "            Y_alt = tf.gather_nd(Y_batch, alt_indices)\n",
    "            g_loss = train_step_gan(X_particle_alt, X_detector_alt, Y_alt)\n",
    "            generator_losses.append(g_loss.numpy())\n",
    "\n",
    "    # Log and printing specs about the model\n",
    "    avg_c_loss = np.mean(critic_losses[-n*len(train_dataset):])\n",
    "    avg_g_loss = np.mean(generator_losses[-m*len(train_dataset):])\n",
    "    gen_loss_avg.append(avg_g_loss)\n",
    "    crit_loss_avg.append(avg_c_loss)   \n",
    "    print(f\"Epoch {epoch+1} completed. Critic Loss: {avg_c_loss}, Generator Loss: {avg_g_loss}\")\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        gen_checkpoint_path = checkpoint_path_gen.format(epoch=epoch + 1)\n",
    "        model_generator.save_weights(gen_checkpoint_path)\n",
    "        print(f'Saved generator at epoch {epoch + 1}')\n",
    "    weights = model_generator.predict(X_test_truth[Y_test == 0], verbose=0).flatten()\n",
    "    performance_metric[epoch, :] = [wasserstein_distance(data_set_1[:, i], data_set_2[:, i], \n",
    "                                                          u_weights=None, v_weights=weights) \n",
    "                                    for i in range(n_variables)]\n",
    "\n",
    "np.savez_compressed('WGAN_training_metrics_compressed.npz', crit_loss_avg=crit_loss_avg, gen_loss_avg=gen_loss_avg, \n",
    "                    baseline=baseline, performance_metric=performance_metric)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d76a13bc-0780-46bb-b60f-ee061bb7aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('WGAN_training_metrics_compressed.npz', crit_loss_avg=crit_loss_avg, gen_loss_avg=gen_loss_avg, baseline=baseline, performance_metric=performance_metric, X_test_truth = X_test_truth, Y_test = Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11053041-b0f1-4110-b54f-8cac1121a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_checkpoint_path = checkpoint_path_gen.format(epoch=epoch + 1)\n",
    "model_generator.save_weights(gen_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5e6eeb0-2ae2-4d4e-a5e7-5bf6a0999cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.788406504665506"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(r_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791d1d70-3a46-42ab-8206-f418f8deb093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3.12.2 (TF, CUDA)",
   "language": "python",
   "name": "python3122"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
