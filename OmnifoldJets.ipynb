{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omnifold import DataLoader, MLP, MultiFold \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm  # Add progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('rawdata_omnifold.npz')\n",
    "ndim = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "observables = [\"m\", \"M\", \"w\", \"tau21\", \"zg\", \"sdm\"]\n",
    "\n",
    "true, reco, true_alt, reco_alt = (\n",
    "    np.column_stack([data[f\"{obs}_{suffix}\"] for obs in observables])\n",
    "    for suffix in [\"true\", \"reco\", \"true_alt\", \"reco_alt\"]\n",
    ")\n",
    "true_train, true_test, reco_train, reco_test,  = train_test_split(\n",
    "    true, reco, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "true_alt_train, true_alt_test, reco_alt_train, reco_alt_test = train_test_split(\n",
    "    true_alt, reco_alt, test_size=0.25, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Creating weights ...\n",
      "INFO: Creating pass reco flag ...\n",
      "INFO: Creating pass gen flag ...\n",
      "INFO: Normalizing sum of weights to 1000000 ...\n",
      "INFO: Creating weights ...\n",
      "INFO: Creating pass reco flag ...\n",
      "INFO: Creating pass gen flag ...\n",
      "INFO: Normalizing sum of weights to 1000000 ...\n"
     ]
    }
   ],
   "source": [
    "nature_loader = DataLoader(\n",
    "            reco = reco_train,\n",
    "            gen = true_train,\n",
    "            bootstrap=False,\n",
    "            normalize=True)\n",
    "mc_loader = DataLoader(\n",
    "            reco = reco_alt_train,\n",
    "            gen = true_alt_train,\n",
    "            bootstrap=False,\n",
    "            normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1741966760.327901 2857293 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4862 training steps at reco and 4942 steps at gen\n",
      "ITERATION: 1\n",
      "RUNNING STEP 1\n",
      "Creating cached data from step 1\n",
      "################################################################################\n",
      "Train events used: 2489344, Test events used: 497868\n",
      "################################################################################\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m      3\u001b[0m gen_model \u001b[38;5;241m=\u001b[39m MLP(ndim, [\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m128\u001b[39m])\n\u001b[1;32m      5\u001b[0m omnifold \u001b[38;5;241m=\u001b[39m MultiFold(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAN Comparison\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m     reco_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5e-5\u001b[39m,\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m \u001b[43momnifold\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUnfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m unfolded_weights  \u001b[38;5;241m=\u001b[39m omnifold\u001b[38;5;241m.\u001b[39mreweight(true_alt_test,omnifold\u001b[38;5;241m.\u001b[39mmodel2,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/omnifold/lib/python3.12/site-packages/omnifold/omnifold.py:156\u001b[0m, in \u001b[0;36mMultiFold.Unfold\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_string(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mITERATION: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRunStep1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mRunStep2(i)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCompileModels(fixed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/omnifold/lib/python3.12/site-packages/omnifold/omnifold.py:165\u001b[0m, in \u001b[0;36mMultiFold.RunStep1\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_string(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRUNNING STEP 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRunModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels_mc\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels_data\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights_push\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpass_reco\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpass_reco\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstepn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNTRAIN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_steps_reco\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcached\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#after first training cache the training data\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m#Don't update weights where there's no reco events\u001b[39;00m\n\u001b[1;32m    177\u001b[0m new_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_pull)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/omnifold/lib/python3.12/site-packages/omnifold/omnifold.py:270\u001b[0m, in \u001b[0;36mMultiFold.RunModel\u001b[0;34m(self, labels, weights, iteration, model, stepn, NTRAIN, cached)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# model_e = model  # TEST of processing iterations w/o ensemble\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCompileModel(model_e, num_steps)\n\u001b[0;32m--> 270\u001b[0m hist \u001b[38;5;241m=\u001b[39m  \u001b[43mmodel_e\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_frac\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mNTRAIN\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNTEST\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_string(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLast val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhist\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrank \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/omnifold/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/omnifold/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/omnifold/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/omnifold/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/omnifold/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/omnifold/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m   filtered_flat_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    915\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(\n\u001b[1;32m    916\u001b[0m           bound_args\n\u001b[1;32m    917\u001b[0m       )\n\u001b[1;32m    918\u001b[0m   )\n\u001b[0;32m--> 919\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    920\u001b[0m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[1;32m    925\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/omnifold/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/omnifold/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/omnifold/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/omnifold/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/omnifold/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ndim = 6\n",
    "reco_model = MLP(ndim, layer_sizes = [128,128,128])\n",
    "gen_model = MLP(ndim, [128,128,128])\n",
    "\n",
    "omnifold = MultiFold(\n",
    "    \"RAN Comparison\",\n",
    "    reco_model,\n",
    "    gen_model,\n",
    "    nature_loader,\n",
    "    mc_loader,\n",
    "    batch_size = 512,\n",
    "    niter = 5,\n",
    "    epochs = 20,\n",
    "    verbose = True,\n",
    "    lr = 5e-5,\n",
    ")\n",
    "omnifold.Unfold()\n",
    "unfolded_weights  = omnifold.reweight(true_alt_test,omnifold.model2,batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  48/1687\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1687/1687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "unfolded_weights  = omnifold.reweight(true_alt,omnifold.model2,batch_size=1000)\n",
    "ran_weights = np.random.normal(np.ones_like(data['m_true']),20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('weights_omnifold.npz', unfolded_weights=unfolded_weights, ran_weights=ran_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('rawdata_omnifold.npz')\n",
    "weights = np.load('weights_omnifold.npz')\n",
    "substructure_variables = ['m', 'M', 'w', 'tau21', 'zg', 'sdm']\n",
    "\n",
    "# a dictionary to hold information about the observables\n",
    "obs = {}\n",
    "\n",
    "# the jet mass and histogram style information\n",
    "obs.setdefault('m', {}).update({\n",
    "    'xlim': (0, 75), 'ylim': (0, 0.065),\n",
    "    'xlabel': r'Jet Mass', 'symbol': r'$m$ [GeV]',\n",
    "    'stamp_xy': (0.425, 0.65),\n",
    "})\n",
    "\n",
    "# the constituent multiplicity and histogram style information\n",
    "obs.setdefault('M', {}).update({\n",
    "    'xlim': (0, 80), 'ylim': (0, 0.065),\n",
    "    'xlabel': 'Jet Constituent Multiplicity', 'symbol': r'$M$',\n",
    "    'stamp_xy': (0.42, 0.65),\n",
    "})\n",
    "\n",
    "# the jet width and histogram style information\n",
    "obs.setdefault('w', {}).update({\n",
    "    'xlim': (0, 0.6), 'ylim': (0, 10),\n",
    "    'xlabel': r'Jet Width', 'symbol': r'$w$',\n",
    "    'stamp_xy': (0.425, 0.65),\n",
    "})\n",
    "\n",
    "# the N-subjettiness ratio and histogram style information\n",
    "obs.setdefault('tau21', {}).update({\n",
    "    'xlim': (0, 1.2), 'ylim': (0, 3),\n",
    "    'xlabel': r'$N$-subjettiness Ratio', 'symbol': r'$\\tau_{21}^{(\\beta=1)}$',\n",
    "    'stamp_xy': (0.41, 0.92),\n",
    "    'legend_loc': 'upper left', 'legend_ncol': 1,\n",
    "})\n",
    "\n",
    "# the groomed momentum fraction and histogram style information\n",
    "obs.setdefault('zg', {}).update({\n",
    "    'xlim': (0, 0.5), 'ylim': (0, 9),\n",
    "    'xlabel': r'Groomed Jet Momentum Fraction', 'symbol': r'$z_g$',\n",
    "    'stamp_xy': (0.425, 0.65),\n",
    "})\n",
    "\n",
    "# the groomed jet mass and histogram style information\n",
    "obs.setdefault('sdm', {}).update({\n",
    "    'xlim': (-14, -2), 'ylim': (0, 0.3),\n",
    "    'xlabel': r'Soft Drop Jet Mass', 'symbol': r'$\\ln\\rho$',\n",
    "    'stamp_xy': (0.41, 0.92),\n",
    "    'legend_loc': 'upper left', 'legend_ncol': 1,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'FUNCTIONS'\n",
    "normalize = lambda x: x / np.sum(x, axis=0)\n",
    "\n",
    "def generate_purity_bins(truth, data, min_purity=0.5, epsilon=1e-6, max_bins_per_dim=15):\n",
    "    \"\"\"\n",
    "    Generate adaptive bins for each dimension based on purity, using predefined min/max limits\n",
    "    \n",
    "    Parameters:\n",
    "    - truth: Truth data with shape (ndim, N)\n",
    "    - data: Reconstructed data with shape (ndim, N)\n",
    "    - min_purity: Minimum purity required for a bin\n",
    "    - epsilon: Small value to avoid numerical issues\n",
    "    - max_bins_per_dim: Maximum number of bins per dimension\n",
    "    \n",
    "    Returns:\n",
    "    - List of bin arrays for each dimension\n",
    "    \"\"\"\n",
    "    ndim = truth.shape[0]\n",
    "    all_bins = []\n",
    "    \n",
    "    # Map dimension indices to variable names\n",
    "    dim_to_var = {\n",
    "        0: 'm',     # Jet mass\n",
    "        1: 'M',     # Multiplicity \n",
    "        2: 'w',     # Width\n",
    "        3: 'tau21', # N-subjettiness\n",
    "        4: 'zg',    # Groomed momentum fraction\n",
    "        5: 'sdm',   # Soft Drop Mass\n",
    "    }\n",
    "    \n",
    "    for dim in range(ndim):\n",
    "        # Get variable name for this dimension\n",
    "        var_name = dim_to_var.get(dim)\n",
    "        \n",
    "        if var_name and var_name in obs:\n",
    "            # Use predefined min/max from the obs dictionary\n",
    "            min_val, max_val = obs[var_name]['xlim']\n",
    "            print(f\"Dimension {dim} ({var_name}): Using predefined range [{min_val}, {max_val}]\")\n",
    "        else:\n",
    "            # Fall back to data min/max if no predefined limits\n",
    "            min_val = truth[dim].min()\n",
    "            max_val = truth[dim].max()\n",
    "            print(f\"Dimension {dim}: Using data range [{min_val:.4f}, {max_val:.4f}]\")\n",
    "        \n",
    "        # Start with the minimum value\n",
    "        bins = [min_val]\n",
    "        i = 0\n",
    "        \n",
    "        # Create adaptive bins based on purity\n",
    "        while bins[-1] < max_val and i < len(bins) and len(bins) < max_bins_per_dim:\n",
    "            for binhigh in np.linspace(bins[i] + epsilon, max_val, 20):\n",
    "                in_bin = (truth[dim] > bins[i]) & (truth[dim] < binhigh)\n",
    "                in_reco_bin = (data[dim] > bins[i]) & (data[dim] < binhigh)\n",
    "                if np.sum(in_bin) > 0:\n",
    "                    purity = np.sum(in_bin & in_reco_bin) / np.sum(in_bin)\n",
    "                    if purity > min_purity:\n",
    "                        i += 1\n",
    "                        bins.append(binhigh)\n",
    "                        break\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        # Ensure the last bin includes the maximum value\n",
    "        if bins[-1] < max_val:\n",
    "            bins.append(max_val)\n",
    "            \n",
    "        all_bins.append(np.array(bins))\n",
    "    \n",
    "    # Print info about number of bins per dimension\n",
    "    print(\"\\nBins per dimension:\")\n",
    "    for dim, bins in enumerate(all_bins):\n",
    "        var_name = dim_to_var.get(dim, f\"Dimension {dim}\")\n",
    "        print(f\"  {var_name}: {len(bins)-1} bins, range: [{bins[0]:.4f}, {bins[-1]:.4f}]\")\n",
    "    \n",
    "    # Calculate total size of the full matrix\n",
    "    total_bins = 1\n",
    "    for bins in all_bins:\n",
    "        total_bins *= (len(bins)-1)\n",
    "    print(f\"Total bins in truth space: {total_bins}\")\n",
    "    \n",
    "    return all_bins\n",
    "\n",
    "def create_response_matrix_nd(gen, sim, bins):\n",
    "    \"\"\"\n",
    "    Create N-dimensional response matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    - gen: Truth data with shape (ndim, N)\n",
    "    - sim: Simulated/reconstructed data with shape (ndim, N)\n",
    "    - bins: List of bin edges for each dimension\n",
    "    \n",
    "    Returns:\n",
    "    - Response matrix mapping from truth to measured space\n",
    "    \"\"\"\n",
    "    ndim = gen.shape[0]\n",
    "    \n",
    "    # Create a list of bins for all 2*ndim dimensions\n",
    "    # First ndim dimensions are for gen, last ndim for sim\n",
    "    bins_list = []\n",
    "    for i in range(ndim):\n",
    "        bins_list.append(bins[i])  # Add gen bins\n",
    "    for i in range(ndim):\n",
    "        bins_list.append(bins[i])  # Add sim bins\n",
    "    \n",
    "    # Stack data for histogramdd: [gen_dim1, gen_dim2, ..., sim_dim1, sim_dim2, ...]\n",
    "    combined_data = np.vstack([gen, sim]).T\n",
    "    \n",
    "    # Create the joint histogram\n",
    "    H, _ = np.histogramdd(combined_data, bins=bins_list)\n",
    "    \n",
    "    # Normalize to get conditional probabilities\n",
    "    # Sum over the sim dimensions (last ndim dimensions)\n",
    "    sum_axes = tuple(range(ndim, 2*ndim))\n",
    "    norm = np.sum(H, axis=sum_axes, keepdims=True)\n",
    "    H_norm = np.divide(H, norm, where=norm>0, out=np.zeros_like(H, dtype=float))\n",
    "    H_norm[np.isnan(H_norm)] = 0\n",
    "    \n",
    "    return H_norm\n",
    "\n",
    "\n",
    "def bayesian_unfolding_step_nd(R, f, data_hist):\n",
    "    \"\"\"\n",
    "    Perform one step of Bayesian unfolding for N-dimensional data.\n",
    "    \n",
    "    Parameters:\n",
    "    - R: Response matrix from create_response_matrix_nd\n",
    "    - f: Current estimate of true distribution\n",
    "    - data_hist: Measured data histogram\n",
    "    \n",
    "    Returns:\n",
    "    - Updated estimate of true distribution\n",
    "    \"\"\"\n",
    "    ndim = len(data_hist.shape)\n",
    "    \n",
    "    # Expected measurements: R applied to f\n",
    "    axes = [list(range(ndim, 2*ndim)), list(range(ndim))]\n",
    "    expected = np.tensordot(R, f, axes=axes)\n",
    "    \n",
    "    # Reweighting factors\n",
    "    reweight = np.divide(data_hist, expected, where=expected>0, out=np.zeros_like(data_hist, dtype=float))\n",
    "    reweight[np.isnan(reweight)] = 0\n",
    "    \n",
    "    # Apply reweighting to get updated f\n",
    "    axes = [list(range(ndim)), list(range(ndim))]\n",
    "    f_prime = f * np.tensordot(R, reweight, axes=axes)\n",
    "    \n",
    "    return f_prime\n",
    "\n",
    "def iterative_bayesian_unfolding_nd(data, gen, sim, bins, n_iterations):\n",
    "    \"\"\"\n",
    "    Perform iterative Bayesian unfolding for N-dimensional data.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Measured data with shape (ndim, N)\n",
    "    - gen: Generated/truth data with shape (ndim, N)\n",
    "    - sim: Simulated/reconstructed data with shape (ndim, N)\n",
    "    - bins: Bin specification for all dimensions\n",
    "    - n_iterations: Number of unfolding iterations\n",
    "    \n",
    "    Returns:\n",
    "    - List of unfolded distributions for each iteration\n",
    "    \"\"\"    \n",
    "    # Create the response matrix\n",
    "    R = create_response_matrix_nd(gen, sim, bins)\n",
    "    \n",
    "    # Initial histogram\n",
    "    f, _ = np.histogramdd(gen.T, bins=bins)\n",
    "    data_hist, _ = np.histogramdd(data.T, bins=bins)\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        f = bayesian_unfolding_step_nd(R, f, data_hist)\n",
    "    \n",
    "    return f\n",
    "\n",
    "def plot_unfolding_results(true_hist, unfolded_hist, bins, dim_to_plot=0, fig=None, ax=None):\n",
    "    \"\"\"\n",
    "    Plot unfolding results for a specified dimension by integrating over other dimensions\n",
    "    \n",
    "    Parameters:\n",
    "    - true_hist: True histogram\n",
    "    - unfolded_hist: Unfolded histogram\n",
    "    - bins: Bin edges used for histograms\n",
    "    - dim_to_plot: Which dimension to plot (0-based index)\n",
    "    \"\"\"\n",
    "    ndim = len(bins)\n",
    "    \n",
    "    # Sum over all dimensions except the one to plot\n",
    "    sum_axes = tuple([i for i in range(ndim) if i != dim_to_plot])\n",
    "    true_hist_1d = np.sum(true_hist, axis=sum_axes)\n",
    "    unfolded_hist_1d = np.sum(unfolded_hist, axis=sum_axes)\n",
    "    \n",
    "    # Normalize histograms\n",
    "    true_hist_1d = true_hist_1d / np.sum(true_hist_1d)\n",
    "    unfolded_hist_1d = unfolded_hist_1d / np.sum(unfolded_hist_1d)\n",
    "    \n",
    "    # Create bin centers for plotting\n",
    "    bin_centers = 0.5 * (bins[dim_to_plot][1:] + bins[dim_to_plot][:-1])\n",
    "    bin_widths = np.diff(bins[dim_to_plot])\n",
    "    \n",
    "    # Create figure if not provided\n",
    "    if fig is None or ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot histograms\n",
    "    ax.bar(bin_centers, true_hist_1d, width=bin_widths, alpha=0.5, label='True')\n",
    "    ax.bar(bin_centers, unfolded_hist_1d, width=bin_widths, alpha=0.5, label='Unfolded')\n",
    "    \n",
    "    ax.set_xlabel(f'Dimension {dim_to_plot}')\n",
    "    ax.set_ylabel('Normalized Counts')\n",
    "    ax.legend()\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension 0 (m): Using predefined range [0, 75]\n",
      "Dimension 1 (M): Using predefined range [0, 80]\n",
      "Dimension 2 (w): Using predefined range [0, 0.6]\n",
      "Dimension 3 (tau21): Using predefined range [0, 1.2]\n",
      "Dimension 4 (zg): Using predefined range [0, 0.5]\n",
      "Dimension 5 (sdm): Using predefined range [-14, -2]\n",
      "\n",
      "Bins per dimension:\n",
      "  m: 15 bins, range: [0.0000, 75.0000]\n",
      "  M: 7 bins, range: [0.0000, 80.0000]\n",
      "  w: 15 bins, range: [0.0000, 0.6000]\n",
      "  tau21: 14 bins, range: [0.0000, 1.2000]\n",
      "  zg: 9 bins, range: [0.0000, 0.5000]\n",
      "  sdm: 15 bins, range: [-14.0000, -2.0000]\n",
      "Total bins in truth space: 2976750\n"
     ]
    }
   ],
   "source": [
    "bins = generate_purity_bins(true_train.T, reco_train.T, min_purity=(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 441. TiB for an array with shape (60562512324864,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m unfolded_hists \u001b[38;5;241m=\u001b[39m \u001b[43miterative_bayesian_unfolding_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreco_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_alt_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreco_alt_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 163\u001b[0m, in \u001b[0;36miterative_bayesian_unfolding_nd\u001b[0;34m(data, gen, sim, bins, n_iterations)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03mPerform iterative Bayesian unfolding for N-dimensional data.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m- List of unfolded distributions for each iteration\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m    \n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Create the response matrix\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m R \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_response_matrix_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Initial histogram\u001b[39;00m\n\u001b[1;32m    166\u001b[0m f, _ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhistogramdd(gen\u001b[38;5;241m.\u001b[39mT, bins\u001b[38;5;241m=\u001b[39mbins)\n",
      "Cell \u001b[0;32mIn[14], line 109\u001b[0m, in \u001b[0;36mcreate_response_matrix_nd\u001b[0;34m(gen, sim, bins)\u001b[0m\n\u001b[1;32m    106\u001b[0m combined_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvstack([gen, sim])\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Create the joint histogram\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m H, _ \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogramdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Normalize to get conditional probabilities\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Sum over the sim dimensions (last ndim dimensions)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m sum_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(ndim, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mndim))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/envs/omnifold/lib/python3.12/site-packages/numpy/lib/histograms.py:1048\u001b[0m, in \u001b[0;36mhistogramdd\u001b[0;34m(sample, bins, range, density, weights)\u001b[0m\n\u001b[1;32m   1044\u001b[0m xy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mravel_multi_index(Ncount, nbin)\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;66;03m# Compute the number of repetitions in xy and assign it to the\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;66;03m# flattened histmat.\u001b[39;00m\n\u001b[0;32m-> 1048\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbincount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnbin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# Shape into a proper matrix\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m hist \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mreshape(nbin)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 441. TiB for an array with shape (60562512324864,) and data type int64"
     ]
    }
   ],
   "source": [
    "unfolded_hists = iterative_bayesian_unfolding_nd(reco_train.T, true_alt_train.T, reco_alt_train.T, bins, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save everything\n",
    "with open('IBU_omnifold.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'bins': bins\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1224331, 6)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reco_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_response_matrix(gen, sim, bins):\n",
    "    \"\"\"\n",
    "    Create a memory-efficient sparse representation of the response matrix\n",
    "    \n",
    "    Parameters:\n",
    "    - gen: Truth data (can be either shape (N, ndim) or (ndim, N))\n",
    "    - sim: Simulated/reconstructed data (same shape as gen)\n",
    "    - bins: List of bin edges for each dimension\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary mapping (gen_idx, sim_idx) to normalized response probability\n",
    "    \"\"\"\n",
    "    # Check and fix data orientation\n",
    "    if gen.shape[0] > gen.shape[1] and len(bins) == gen.shape[1]:\n",
    "        # Data is in shape (N, ndim), so transpose to (ndim, N)\n",
    "        print(f\"Transposing data from shape {gen.shape} to {gen.shape[::-1]}\")\n",
    "        gen = gen.T\n",
    "        sim = sim.T\n",
    "    \n",
    "    ndim = gen.shape[0]\n",
    "    n_events = gen.shape[1]\n",
    "    \n",
    "    # Verify dimensions match\n",
    "    if len(bins) != ndim:\n",
    "        raise ValueError(f\"Number of bin arrays ({len(bins)}) doesn't match data dimensions ({ndim})\")\n",
    "    \n",
    "    print(f\"Creating sparse response matrix for {ndim}D data with {n_events} events\")\n",
    "    print(f\"Bins shape: {[len(b) for b in bins]}\")\n",
    "    \n",
    "    # Create dictionaries to store counts\n",
    "    joint_counts = defaultdict(int)\n",
    "    gen_counts = defaultdict(int)\n",
    "    \n",
    "    # Process events in batches to avoid memory issues\n",
    "    batch_size = 10000\n",
    "    for batch_start in tqdm(range(0, n_events, batch_size), desc=\"Processing batches\"):\n",
    "        batch_end = min(batch_start + batch_size, n_events)\n",
    "        \n",
    "        # Calculate bin indices for this batch\n",
    "        gen_indices = []\n",
    "        sim_indices = []\n",
    "        \n",
    "        for dim in range(ndim):\n",
    "            gen_idx = np.digitize(gen[dim, batch_start:batch_end], bins[dim]) - 1\n",
    "            sim_idx = np.digitize(sim[dim, batch_start:batch_end], bins[dim]) - 1\n",
    "            \n",
    "            # Clip indices to valid range\n",
    "            gen_idx = np.clip(gen_idx, 0, len(bins[dim])-2)\n",
    "            sim_idx = np.clip(sim_idx, 0, len(bins[dim])-2)\n",
    "            \n",
    "            gen_indices.append(gen_idx)\n",
    "            sim_indices.append(sim_idx)\n",
    "        \n",
    "        # Count events in each (gen, sim) bin combination\n",
    "        for i in range(batch_end - batch_start):\n",
    "            gen_idx = tuple(gen_indices[d][i] for d in range(ndim))\n",
    "            sim_idx = tuple(sim_indices[d][i] for d in range(ndim))\n",
    "            \n",
    "            joint_counts[(gen_idx, sim_idx)] += 1\n",
    "            gen_counts[gen_idx] += 1\n",
    "    \n",
    "    # Normalize to get conditional probabilities P(sim|gen)\n",
    "    print(\"Normalizing response matrix...\")\n",
    "    response_dict = {}\n",
    "    for (gen_idx, sim_idx), count in joint_counts.items():\n",
    "        gen_count = gen_counts[gen_idx]\n",
    "        if gen_count > 0:\n",
    "            response_dict[(gen_idx, sim_idx)] = count / gen_count\n",
    "    \n",
    "    print(f\"Response matrix size: {len(response_dict)} non-zero elements\")\n",
    "    return response_dict, gen_counts\n",
    "def create_histogram_nd(data, bins):\n",
    "    \"\"\"Create N-dimensional histogram using batch processing\"\"\"\n",
    "    # Check and fix data orientation\n",
    "    if data.shape[0] > data.shape[1] and len(bins) == data.shape[1]:\n",
    "        # Data is in shape (N, ndim), so transpose to (ndim, N)\n",
    "        print(f\"Transposing data from shape {data.shape} to {data.shape[::-1]}\")\n",
    "        data = data.T\n",
    "    \n",
    "    ndim = data.shape[0]\n",
    "    n_events = data.shape[1]\n",
    "    \n",
    "    # Verify dimensions match\n",
    "    if len(bins) != ndim:\n",
    "        raise ValueError(f\"Number of bin arrays ({len(bins)}) doesn't match data dimensions ({ndim})\")\n",
    "    \n",
    "    # Initialize dict for histogram counts\n",
    "    hist_dict = defaultdict(int)\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_size = 10000\n",
    "    for batch_start in tqdm(range(0, n_events, batch_size), desc=\"Creating histogram\"):\n",
    "        batch_end = min(batch_start + batch_size, n_events)\n",
    "        \n",
    "        # Calculate bin indices for each dimension\n",
    "        indices = []\n",
    "        for dim in range(ndim):\n",
    "            idx = np.digitize(data[dim, batch_start:batch_end], bins[dim]) - 1\n",
    "            idx = np.clip(idx, 0, len(bins[dim])-2)\n",
    "            indices.append(idx)\n",
    "        \n",
    "        # Increment counts\n",
    "        for i in range(batch_end - batch_start):\n",
    "            bin_idx = tuple(indices[d][i] for d in range(ndim))\n",
    "            hist_dict[bin_idx] += 1\n",
    "    \n",
    "    return hist_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_bayesian_unfolding_sparse(data, gen, sim, bins, n_iterations):\n",
    "    \"\"\"\n",
    "    Perform IBU with sparse matrices for memory efficiency\n",
    "    \n",
    "    Parameters:\n",
    "    - data: Measured data to unfold\n",
    "    - gen: Generated/truth training data\n",
    "    - sim: Simulated/reconstructed training data \n",
    "    - bins: List of bin edges for each dimension\n",
    "    - n_iterations: Number of unfolding iterations\n",
    "    \n",
    "    Returns:\n",
    "    - Dense array of unfolded distribution after final iteration\n",
    "    \"\"\"\n",
    "    # Print shapes to debug\n",
    "    print(f\"Data shape: {data.shape}\")\n",
    "    print(f\"Gen shape: {gen.shape}\")\n",
    "    print(f\"Sim shape: {sim.shape}\")\n",
    "    print(f\"Number of bin arrays: {len(bins)}\")\n",
    "    \n",
    "    # Create sparse response matrix\n",
    "    response_dict, gen_counts = create_sparse_response_matrix(gen, sim, bins)\n",
    "    \n",
    "    # Create initial histograms\n",
    "    print(\"Creating initial histograms...\")\n",
    "    f_dict = create_histogram_nd(gen, bins)\n",
    "    data_dict = create_histogram_nd(data, bins)\n",
    "    \n",
    "    # Perform iterative unfolding\n",
    "    print(\"Starting iterative unfolding...\")\n",
    "    for iter_idx in range(n_iterations):\n",
    "        print(f\"Iteration {iter_idx+1}/{n_iterations}\")\n",
    "        f_dict = bayesian_unfolding_step_sparse(response_dict, gen_counts, f_dict, data_dict)\n",
    "    \n",
    "    # Convert final result to dense array\n",
    "    print(\"Converting result to dense array...\")\n",
    "    result = sparse_to_dense(f_dict, bins)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reco_test shape: (408111, 6)\n",
      "true_train shape: (1224331, 6)\n",
      "reco_train shape: (1224331, 6)\n",
      "Number of bin arrays: 6\n",
      "Data shape: (408111, 6)\n",
      "Gen shape: (1265217, 6)\n",
      "Sim shape: (1265217, 6)\n",
      "Number of bin arrays: 6\n",
      "Transposing data from shape (1265217, 6) to (6, 1265217)\n",
      "Creating sparse response matrix for 6D data with 1265217 events\n",
      "Bins shape: [16, 8, 16, 15, 10, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 127/127 [00:06<00:00, 20.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing response matrix...\n",
      "Response matrix size: 1157966 non-zero elements\n",
      "Creating initial histograms...\n",
      "Transposing data from shape (1265217, 6) to (6, 1265217)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating histogram: 100%|██████████| 127/127 [00:02<00:00, 51.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposing data from shape (408111, 6) to (6, 408111)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating histogram: 100%|██████████| 41/41 [00:00<00:00, 51.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iterative unfolding...\n",
      "Iteration 1/5\n",
      "Iteration 2/5\n",
      "Iteration 3/5\n",
      "Iteration 4/5\n",
      "Iteration 5/5\n",
      "Converting result to dense array...\n",
      "Transposing data from shape (408111, 6) to (6, 408111)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating histogram: 100%|██████████| 41/41 [00:00<00:00, 51.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# Print shapes to debug before calling\n",
    "print(f\"reco_test shape: {reco_test.shape}\")\n",
    "print(f\"true_train shape: {true_train.shape}\")\n",
    "print(f\"reco_train shape: {reco_train.shape}\")\n",
    "print(f\"Number of bin arrays: {len(bins)}\")\n",
    "\n",
    "# Run the unfolding (will handle transposition if needed)\n",
    "unfolded_hist = iterative_bayesian_unfolding_sparse(\n",
    "    reco_test,\n",
    "    true_alt_train,\n",
    "    reco_alt_train,\n",
    "    bins,\n",
    "    n_iterations=5\n",
    ")\n",
    "\n",
    "# Create true test histogram for comparison\n",
    "true_test_hist = sparse_to_dense(create_histogram_nd(true_test, bins), bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('IBU_omnifold.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'unfolded_hist': unfolded_hist,  # Make sure this key exists!\n",
    "        'true_test_hist': true_test_hist,\n",
    "        'bins': bins\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnifold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
